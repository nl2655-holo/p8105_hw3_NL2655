---
title: "HW3"
author: "Nankun"
date: "2019/10/13"
output: github_document
---

# Homework 3

## Library
```{r}
library(tidyverse)
library(ggridges)
library(patchwork)
library(readr)
library(p8105.datasets)
```
## Problem 1
### Number of aisle and the most popular one
```{r}
data("instacart")
ais_num = 
  max(pull(instacart,aisle_id))
ais_num
pop_ais=
instacart %>% 
  group_by(aisle) %>% 
  summarize(
    n_ais = n()
  ) %>% 
  arrange(desc(n_ais)) %>% 
  filter(min_rank(desc(n_ais))<2) %>% 
  pull(aisle)
pop_ais
```

**Answer**:
There are `r  max(pull(instacart,aisle_id))` aisle and fresh vegetable is the most item ordered from.

### Plot
```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(
    n_ais = n()
  ) %>% 
  filter(n_ais > 10000) %>% 
  ggplot(aes(x = aisle, y = n_ais)) +
  geom_point() +
  labs(
    title = "Aisle orders plot",
    x = "Aisle names",
    y = "Order amount",
    caption = "Data from instacart"
  ) +
  ggthemes::theme_excel()+
  theme(axis.text.x = element_text(angle = 90,hjust=1))
```

**Answer**:
The plot is showing above, including the order count for all aisles with more than 10000 items ordered.

### 3 most popular in three aisles
```{r}
instacart %>% 
  filter(aisle == "baking ingredients"|aisle ==  "dog food care"|aisle ==  "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(
    sum = sum(order_number)
  ) %>% 
filter(min_rank(desc(sum))<=3) %>% 
  arrange(desc(sum)) %>% 
 knitr::kable()
```

**Answer**:
Table is showing above, including the 3 most popular items in “baking ingredients”, “dog food care”, and “packaged vegetables fruits” aisle.

### Table
```{r}
  instacart %>% 
  filter(product_name == "Pink Lady Apples"|product_name ==  "Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>% 
  summarize(
   ohd_mean = mean(order_hour_of_day)
  ) %>% 
   mutate(
     order_dow = recode(order_dow, "0" = "Sun", "1" = "Mon", "2" = "Tue", "3" = "Wed", "4" = "Thu", "5" = "Fri", "6" = "Sat"),
     ohd_mean = round(ohd_mean, 2)
     ) %>% 
    pivot_wider(
    names_from = order_dow,
    values_from = ohd_mean
  ) %>% 
   knitr::kable()
```

**Answer**:
The table is showing above, including the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

**Description**:
Data *instacart* from *library(p8105.datasets)* contains `r nrow(instacart)` observations with `r length(names(instacart))` variables about the order status of items.The key varibles we used in data analysis includes *aisle_id*, *aisle*, *product_nam*, *order_dow*, *order_hour_of_day*, and *order_number*. These variables are analysed to get aisle number, the aisle which most items are ordered from, making plots about the number of items ordered in each aisle with more than 10000 items ordered, making table about the most popular items in “baking ingredients”, “dog food care”, and “packaged vegetables fruits” aisles, and making table showing that mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

## Problem 2
### Data cleaning
```{r}
data("brfss_smart2010")
brfss_td = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(
    topic == "Overall Health") %>% 
    filter(response=="Excellent"|response=="Very good"|
           response=="Good"|response=="Fair"|
           response=="Poor") %>% 
  mutate(
    response = fct_relevel(response, "Poor", "Fair", "Good", "Very good", "Excellent")
  )
brfss_td
```

### State observed
```{r}
brfss_td %>% 
  filter(year==2010|year==2002) %>% 
  group_by(year, locationabbr,locationdesc) %>% 
  summarize(
    n = n()
  ) %>% 
    count(locationabbr,name = "obsd") %>% 
    filter(obsd >= 7) %>% 
  pivot_wider(
    names_from = year,
    values_from = obsd
  )
```

*Answer*:
The states observed equal or more than 7 times in both 2002 and 2010 are shown above.There are 6 state have observed equal or more than 7 times in 2002, but much more in 2010.

### spaghetti plot
```{r}
brfss_td %>% 
  filter(response == "Excellent") %>% 
  group_by(year, locationabbr, locationdesc) %>% 
  summarize(
    ave_val = mean(data_value, na.rm = TRUE),
  ) %>% 
  ggplot(aes(x = year, y = ave_val)) + 
  geom_point() + 
  geom_line(aes(group = locationabbr))+
  labs(
    y="average of data value",
    title = "average value over time within a state"
  )
```

*Answer*:
“spaghetti” plot of this average value over time within a state is made above.

### two panel plot
```{r}
plot_2006 =
  brfss_td %>%
  filter(year == "2006") %>% 
  ggplot(aes(x = response, y = data_value)) + 
  geom_violin(aes(fill = response), color = "blue", alpha = .5) + 
  labs(
    title = "2006 distribution of data value",
    y = "data value"
    )+
  theme(legend.position = "none")
plot_2010 =
  brfss_td %>%
  filter(year == "2010") %>% 
  ggplot(aes(x = response, y = data_value)) + 
  geom_violin(aes(fill = response), color = "blue", alpha = .5) +
  labs(
    title = "2010 distribution of data value",
    y = "data value"
    )+
  theme(legend.position = "none")
plot_2006 / plot_2010
```

*Answer*:
Two-panel plot is above showing that for the years 2006, and 2010, distribution of *data_value* for responses (“Poor” to “Excellent”) among locations in NY State, by patchwork. We can see that the plot in 2006 has similiar distribution comparing to plot in 2010.

## Problem 3
### data import and cleaning
```{r}
accel = read_csv("data/accel_data.csv")
accel_td = 
  accel %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "min_act",
    names_prefix = "activity_",
    values_to = "activity_counts") %>% 
  mutate(
    weekday = recode(day,
      "Friday" = "Weekday",
      "Monday" = "Weekday",
      "Tuesday" = "Weekday",
      "Wednesday" = "Weekday",
      "Thursday" = "Weekday",
      "Saturday" = "Weekend",
      "Sunday" = "Weekend"
    )
  )
accel_td
accel_td %>% 
  count(day, weekday)
```

*Description*
Data *accel_td* from *accel_data.csv* contains `r nrow(accel_td)` observations with `r length(names(accel_td))` variables about the activity counts for each minute of a 24-hour day starting at midnight for a observer aged 63 and BMI 25.The key varibles we used in data analysis includes *week*, *day_id*, *day*, *min_act*, *activity_counts*, *weekday*, and created variable *day_act_counts*. These variables are analysed to reveal the trends of dayily activity counts.

### Traditional analyses
```{r}
accel_day =
accel_td %>% 
  group_by(day_id, day, weekday) %>% 
  summarize(
    day_act_counts = sum(activity_counts)
  )
  knitr::kable(accel_day)
```

*Trend*
I do not see much trend here, the only trend I see is that the Saturday counts in last two weeks are very low.

### plot
```{r}
accel_day %>% 
  ggplot(aes(x = day_id, y = day_act_counts)) + 
  geom_point(aes(color = day), alpha = .5)+
  geom_line(aes(color = day))+
  labs(
    title = "activity by day plot",
    x = "day",
    y = "activity counts",
    caption = "from accel data"
  )+
   scale_y_continuous(
    breaks = c(0, 200000, 400000, 600000), 
    labels = c("0", "20k", "40k", "60k"))
```

*Trend*
From plot, we can see that the activity counts in weekdays are kept almost the same, but the counts of activity in weekends drop obveriously.